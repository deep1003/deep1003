{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdcJB8nKqDV+o22ohBkecc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep1003/deep1003/blob/master/Week%2014.%20Patent%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgGglIiKJkDj"
      },
      "outputs": [],
      "source": [
        "#1 Import the necessary packages and modules\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "tqdm.pandas()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from pprint import pprint\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim import corpora, models, similarities\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import KeyedVectors\n",
        "from random import seed, sample\n",
        "import random\n",
        "from ksvd import ApproximateKSVD\n",
        "import sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "# from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "# from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
        "matplotlib_axes_logger.setLevel('ERROR')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Data loading function, to convert the string in each cell of the dataframe to a list\n",
        "def string_to_ls(text):\n",
        "word_ls_strip = []\n",
        "    word_ls = text.strip('][').split(', ')     for w in word_ls:\n",
        "word_ls_strip.append(w.strip(\"'\"))\n",
        "return (word_ls_strip)\n"
      ],
      "metadata": {
        "id": "wvrjvVllKcRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Download the patents data\n",
        "\n",
        "# First, download the patent abstracts from https://patentsview.org/download/data-download-tables\n",
        "# Then, read the dataset into the memory\n",
        "patent_df = pd.read_csv(\"patent.tsv\", low_memory=False, sep=\"\\t\")\n",
        "print(patent_df.shape)\n",
        "patent_df.head()"
      ],
      "metadata": {
        "id": "qW56qVq-KlNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Create the function to clean the text content in the abstract\n",
        "\n",
        "def string_tokenize(text):\n",
        "workingIter = []\n",
        "if isinstance(text, str)==True:\n",
        "    tokenLst    = nltk.word_tokenize(text) # tokenize\n",
        "    workingIter = [w.lower() for w in tokenLst if w.isalpha()] # Lowercasing, remove speacial characters and numbers\n",
        "    return (workingIter)\n",
        ""
      ],
      "metadata": {
        "id": "GXZ7xqltKumx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Create a new column to store the pre-processed tokens and save the data\n",
        "patent_df[\"abstract_token\"] = patent_df[\"abstract\"].progress_apply(lambda x: string_tokenize(x))\n",
        "patent_df = patent_df[['id', 'type', 'number', 'date', 'title', 'abstract_token']]\n",
        "patent_df.to_csv(\"patents_processed_tokens.csv\", index=False)\n",
        "print(patent_df.shape)\n",
        "patent_df.head()\n"
      ],
      "metadata": {
        "id": "5y3tP7d4LDqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Load the processed data from above\n",
        "\n",
        "patent_df = pd.read_csv(\"patents_processed_tokens.csv\", low_memory=False)\n",
        "# Clean string the string of text column\n",
        "patent_df[\"abstract_token\"] = patent_df[\"abstract_token\"].progress_apply(lambda x: string_to_ls(x))\n"
      ],
      "metadata": {
        "id": "Oyz7MA0gLPbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Get the training corpus (all patent text)\n",
        "training_patent = list(patent_df[\"abstract_token\"])\n"
      ],
      "metadata": {
        "id": "STtdhIALLXt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Train the model (use the default setting min_count=5, threshold=10)\n",
        "phrase_model = Phrases(training_patent, onnector_words=ENGLISH_CONNECTOR_WORDS)\n"
      ],
      "metadata": {
        "id": "FCsvFPEPLcpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Get the bigram version of each abstract\n",
        "patent_df[\"abstract_token_bigram\"]=patent_df[\"abstract_token\"].progress_apply(lambda x: phrase_model[x])"
      ],
      "metadata": {
        "id": "YQRTZ1_OLjyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Keep only the necessary columns and save the dataframe\n",
        "patent_df = patent_df[['id', 'type', 'number', 'date', 'title', 'abstract_token_bigram']]"
      ],
      "metadata": {
        "id": "hZpS_c8YLooP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11 Load the processed data from above\n",
        "patent_df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "# clean string of list to list\n",
        "patent_df[\"abstract_token_bigram\"] = patent_df[\"abstract_token_bigram\"].progress_apply(lambda x: string_to_ls(x))"
      ],
      "metadata": {
        "id": "tRDSuj0CLss4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 Convert the column with abstracts into a list to be fed into the algorithm\n",
        "trained_bigram = list(patent_df[\"abstract_token_bigram\"])"
      ],
      "metadata": {
        "id": "0M5QsYIaPCef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13 Set the model parameters and train the model by itreating through the abstracts in the abstract list\n",
        "\n",
        "# Model parameters\n",
        "VectorSize = 300 # the number of dimensions into which each word will be embedded.\n",
        "Window = 5 # The number of words before and after the focal word used to train the embedding.\n",
        "Epochs = 10 # The number of iterations through the corpus that the algorithm will perform to train the model.\n",
        "MinCount = 1 # Tells the algorithm to ignore words with a frequency lower than this.\n",
        "Workers = 6 # My machine has 8 processors, so I am assigning six of these to the task of training the model.\n",
        "# The default algorithm is CBOW. If we wanted to run the skip-gram algorithm, we would have set sg=1 below\n",
        "start = time.time()\n",
        "w2v_model = Word2Vec(sentences=trained_bigram, vector_size=VectorSize, window=Window, min_count=MinCount, workers=Workers, epochs=Epochs)\n",
        "print(\"Minutes it took to train the model: \", (time.time() - start) / 60)\n",
        "w2v_model.save('patentAbstractsW2V_300_10_5.model')"
      ],
      "metadata": {
        "id": "7mnzjBAzPLRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14 Once the model is trained and saved to the hard drive, we can simply read in the model for subsequent use.\n",
        "model = Word2Vec.load('patentAbstractsW2V_300_10_5.model')"
      ],
      "metadata": {
        "id": "qXb5jSZbPgNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15 As a first step in validating the model, we want to see whether the local conceptual structure aligns with our intuition.\n",
        "# To check this, we will ask for the most similar words of a few different concepts.\n",
        "# The code below returns a list of the top ten most similar words to our focal word, including the distance to each of these\n",
        "# words in terms of cosine similarity. The closer the cosine is to 1, the more similar the word is to the focal word.\n",
        "# A cosine of 0 indicates that the word is orthogonal.\n",
        "print(model.wv.most_similar('light'))\n",
        "print(\"\")\n",
        "print(model.wv.most_similar('chemical'))\n",
        "print(\"\")\n",
        "print(model.wv.most_similar('car'))\n",
        "# The output suggests that our model has indeed generated an embedding model that makes sense at the local conceptual level."
      ],
      "metadata": {
        "id": "daE394Z3PoXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16 We can then check whether the local structure is also respecting more global structure across the entire conceptual\n",
        "# space. To do so, we can see whether words that we know to be distant are also distant within the space itself.\n",
        "print(model.wv.similarity('chemical', 'biological'))\n",
        "print(model.wv.similarity('chemical', 'drug'))\n",
        "print(model.wv.similarity('chemical', 'food'))\n",
        "print(model.wv.similarity('chemical', 'engineering'))\n",
        "print(model.wv.similarity('chemical', 'software'))\n",
        "print(model.wv.similarity('chemical', 'car'))"
      ],
      "metadata": {
        "id": "mLk65o-NPr32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17 To more fully check that local and global structure are in alignment with intuition, we can create a 2-dimensional\n",
        "# visualization that plots the most similar terms of different kinds of words. If both local and global distnaces are being\n",
        "# respected, the most similar terms should cluster together while the clusters themselves should be further away from each\n",
        "# other in proportion to how similar they themselves are.\n",
        "keys = ['computer', 'telephone', 'car', 'boat', 'drug', 'chemical']\n",
        "embedding_clusters = []\n",
        "word_clusters = []\n",
        "for word in keys:\n",
        "embeddings = []\n",
        "words = []\n",
        "for similar_word, _ in model.wv.most_similar(word, topn=6):\n",
        "words.append(similar_word)\n",
        "embeddings.append(model.wv[similar_word])\n",
        "embedding_clusters.append(embeddings)\n",
        "word_clusters.append(words)\n",
        "perp = 9\n",
        "embedding_clusters = np.array(embedding_clusters)\n",
        "n, m, k = embedding_clusters.shape\n",
        "tsne_model_en_2d = TSNE(perplexity=perp, n_components=2, init='pca', n_iter=50000, random_state=32)\n",
        "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
        "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
        "plt.figure(figsize=(16, 9))\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
        "for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
        "x = embeddings[:, 0]\n",
        "y = embeddings[:, 1]\n",
        "plt.scatter(x, y, c=color, alpha=a, label=label, s=400)\n",
        "for i, word in enumerate(words):\n",
        "plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 8),\n",
        "textcoords='offset points', ha='right', va='bottom', size=16)\n",
        "plt.legend(loc=4, prop={'size': 16})\n",
        "plt.title(title, fontsize=16)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "#plt.grid(True)\n",
        "plt.grid(False)\n",
        "if filename:\n",
        "plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "outfile = \"G:\\\\My Drive\\\\Projects\\\\0 Word Embeddings OS\\\\Embeddings Appendix\\\\TSNE.png\"\n",
        "tsne_plot_similar_words('Word Similarities from Patent Abstracts', keys, embeddings_en_2d, word_clusters, 0.7, outfile)\n",
        "# The plot indeed shows that the local and global structure are being respected, with the most similar words for the focal\n",
        "# word being located within the same cluster, with similar clusters being near each other (e.g., the computer cluster is near\n",
        "# the telephone cluster, the car cluster is near boat cluster, and the drug cluster is near the chemical cluster)."
      ],
      "metadata": {
        "id": "PUeHdrAaPyB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18 The above graph was induced from the data and the model, jiving with our intuition. But we may also want to check whether\n",
        "# the model output also maps on to previously identified structure. Here we use a graph in Leydesdorff et al. 2014, composed\n",
        "# of the mapping of patent categories through citations. As we can see in figure 2 of the paper, the model's output is in\n",
        "# agreement here as well.\n",
        "print((- (model.wv.similarity('cement', 'nanotechnology') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'plastics') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'yarn') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'spraying') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'water') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'printing') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'lighting') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'vehicles') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'veterinary') + 1) * 50) + 100)\n",
        "# Note that here we have amended the -1 to 1 continuum so that values range between 0 and 100, with larger values equaling\n",
        "# greater distances. Doing so can facilitate the interpretation of regression coefficients when our theoretical construct of\n",
        "# interest in distance instead of similarity.\n"
      ],
      "metadata": {
        "id": "B2_TaZ3QP3pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19 Thus far, we have been validating our model against our intuition or other measures of structure. Yet, we als owant to\n",
        "# ensure that our model is capturing the objective regularities of the physical world. In the context of patents, we can\n",
        "# check whether known distances in the physical world are also represented by the model by seeing whether the table of\n",
        "# elements can be recreated (see figure XX in the paper).\n",
        "print((- (model.wv.similarity('hydrogen', 'lithium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'sodium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'potassium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'rubidium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'caesium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'francium') + 1) * 50) + 100)"
      ],
      "metadata": {
        "id": "wIAgyFwBQPGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Another relevant set of knowledge we would want the embedding model of the patents space to represent is that having to do\n",
        "# with social relations. To check this, we can see how certain concepts project onto relevant social dimensions.\n",
        "# Here we will use gender as a relevant dimension to see whether concepts are correctly projecting onto the female/male\n",
        "# continuum.\n",
        "def normalize(vector):\n",
        "normalized_vector = vector / np.linalg.norm(vector)\n",
        "return normalized_vector\n",
        "def dimension(model, positives, negatives):\n",
        "diff = sum([normalize(model.wv[x]) for x in positives]) - sum([normalize(model.wv[y]) for y in negatives])\n",
        "return diff\n",
        "Gender = dimension(model, ['man','him', 'he', 'male', 'men'], ['woman', 'her', 'she', 'female', 'women'])\n",
        "Concepts = ['dietitian', 'hygienist', 'lipstick', 'breastpump', 'tampon',\n",
        "'military', 'farming', 'police', 'hammer', 'fishing']\n",
        "def makeDF(model, word_list):\n",
        "g = []\n",
        "r = []\n",
        "c = []\n",
        "for word in word_list:\n",
        "g.append(sklearn.metrics.pairwise.cosine_similarity(model.wv[word].reshape(1,-1), Gender.reshape(1,-1))[0][0])\n",
        "df = pd.DataFrame({'gender': g}, index = word_list)\n",
        "return df\n",
        "df = makeDF(model, Concepts)\n",
        "df = df.sort_values(by=['gender'])\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "GT8D5sslQGZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21 We also know that embedding models embed sufficiant relational information to solve analogical reasoning tasks.\n",
        "# Therefore, if our model is well-trained, it should be able to do the same. Table 1 in the paper presents the output of the\n",
        "# code below.\n",
        "print(model.wv.most_similar_cosmul(positive=['tiny', 'large'], negative=['big'])[0]) # Synonym\n",
        "print(model.wv.most_similar_cosmul(positive=['downward', 'liquid'], negative=['solid'])[0]) # Antonym\n",
        "print(model.wv.most_similar_cosmul(positive=['car', 'animal'], negative=['cat'])[0]) #Type\n",
        "print(model.wv.most_similar_cosmul(positive=['skate', 'airplane'], negative=['fly'])[1]) #Type\n",
        "print(model.wv.most_similar_cosmul(positive=['hammer', 'cut'], negative=['knife'])[0]) # Item / Purpose\n",
        "print(model.wv.most_similar_cosmul(positive=['farmer', 'patient'], negative=['doctor'])[0]) # product / worker\n",
        "print(model.wv.most_similar_cosmul(positive=['short', 'longest'], negative=['long'])[0]) # Distance\n",
        "print(model.wv.most_similar_cosmul(positive=['spoon', 'pens'], negative=['pen'])[0]) # Count\n"
      ],
      "metadata": {
        "id": "oAgL4lTeQaNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Load the dataframe with the patent abstracts and load the embedding model\n",
        "df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "df = df.head() # For our practical application examples, we will only use five abstracts\n",
        "df[\"abstract_token_bigram\"] = df[\"abstract_token_bigram\"].progress_apply(lambda x: string_to_ls(x))\n",
        "model = Word2Vec.load('patentAbstractsW2V_300_10_5.model')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "NZ1pDhRsQjyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23 I will print each abstract so that you can see what the text of each abstract states\n",
        "abstract_list = df['abstract_token_bigram'].tolist()\n",
        "for abstract in abstract_list:\n",
        "print(\"\")\n",
        "print(len(abstract))\n",
        "print(abstract)\n"
      ],
      "metadata": {
        "id": "7bUKPJq8Qpd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Measuring the conceptual breadth of sets of words\n",
        "# Because larger values equal greater breadth, we transform the measure to reflect this\n",
        "l1 = [\"chemistry\", \"biochemistry\", \"analytical_chemistry\"]\n",
        "l2 = [\"chemistry\", \"oceanography\", \"computer\"]\n",
        "sims1 = [model.wv.similarity(l1[0], l1[1]), model.wv.similarity(l1[0], l1[2]), model.wv.similarity(l1[1], l1[2]),]\n",
        "sims1mean = (((sum(sims1) / float(len(sims1)) + 1) * -50) + 100) # higher values equal broader distances\n",
        "print(l1)\n",
        "print(sims1mean)\n",
        "print(\"\")\n",
        "sims2 = [model.wv.similarity(l2[0], l2[1]), model.wv.similarity(l2[0], l2[2]), model.wv.similarity(l2[1], l2[2]),]\n",
        "sims2mean = (((sum(sims2) / float(len(sims2)) + 1) * -50) + 100) # higher values equal broader distances\n",
        "print(l2)\n",
        "print(sims2mean)\n",
        "\n"
      ],
      "metadata": {
        "id": "BWYMMTiWQs4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25 This function samples 100 random word pairs from a document and returns their mean value. This can be useful in situations\n",
        "# where there are many documents with many words, such that taking the similarity of all word pairs for all documents\n",
        "# would become computationally time intensive. In the case when it is not computationally intensive, researchers can simply\n",
        "# compute the mean breadth of of the lower triangular matrix of the word-by-word cosine distances below the diagonal.\n",
        "def conceptual_breadth(text): # Calculates the average similarity for 100 random pairs of words\n",
        "numUnique = len(text)\n",
        "distances = []\n",
        "for i in range(100):\n",
        "try:\n",
        "rand1 = text[random.randrange(0, numUnique)] # numUnique is the number of words in the abstract\n",
        "rand2 = text[random.randrange(0, numUnique)]\n",
        "dist = (((model.wv.similarity(rand1, rand2) + 1) * -50) + 100) # higher values equal broader distances\n",
        "distances.append(dist)\n",
        "except:\n",
        "continue\n",
        "try:\n",
        "dists = distances\n",
        "dists.sort(reverse=True)\n",
        "breadth = float(sum(dists)) / float(len(dists))\n",
        "except:\n",
        "breadth = np.nan\n",
        "return breadth\n",
        "df['abstract_breadth'] = df['abstract_token_bigram'].apply(conceptual_breadth)\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "ePgZbncTQ2DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26 Load the dataframe with the patent abstracts and load the embedding model\n",
        "df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "df[\"abstract_token_bigram\"] = df[\"abstract_token_bigram\"].progress_apply(lambda x: string_to_ls(x))\n",
        "model = Word2Vec.load('patentAbstractsW2V_300_10_5.model')\n",
        "model_vocab=model.wv.key_to_index\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "1BOcand7Q8f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27 Function to get the centroid vector of an abstract text (average over all words/bigrams)\n",
        "def get_centroid_vector(token_ls):\n",
        "total_vector=list()\n",
        "for token in token_ls:\n",
        "if token in model_vocab:\n",
        "v=model.wv[token]\n",
        "total_vector.append(v)\n",
        "if len(total_vector)>0:\n",
        "centroid_vector=sum(total_vector)/len(total_vector)\n",
        "else:\n",
        "centroid_vector=np.nan\n",
        "return (centroid_vector)\n",
        "# Below is a faster function to calculate cosine similarity\n",
        "def alt_cosine(x,y):\n",
        "return np.inner(x,y)/np.sqrt(np.dot(x,x)*np.dot(y,y))\n"
      ],
      "metadata": {
        "id": "zTwYcisJQ_WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28 Get the centroid vector for each abstract in table 2\n",
        "# For our example, we will only calculate the centroid of the abstracts we want\n",
        "df = df.loc[df['id'].isin([10000000, 8406638, 10000142])]\n",
        "df['centroid_vector'] = df['abstract_token_bigram'].progress_apply(lambda x: get_centroid_vector(x))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "iNa03M6SREy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29 Compare the similarities for the three patent abstracts in table 2\n",
        "v1 = df.loc[df['id'] == 10000000, 'centroid_vector'].iloc[0]\n",
        "v2 = df.loc[df['id'] == 8406638 , 'centroid_vector'].iloc[0]\n",
        "v3 = df.loc[df['id'] == 10000142, 'centroid_vector'].iloc[0]\n",
        "print(alt_cosine(v1,v2))\n",
        "print(alt_cosine(v1,v3))\n",
        "print(alt_cosine(v3,v2))\n",
        "\n"
      ],
      "metadata": {
        "id": "I1HTAjsDRH9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30 Load the processed data from above\n",
        "df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "df = df[[\"abstract_token_bigram\", \"id\"]]\n",
        "# clean string of list to list\n",
        "df[\"abstract_token_bigram\"] = df[\"abstract_token_bigram\"].progress_apply(lambda x: string_to_ls(x))\n"
      ],
      "metadata": {
        "id": "5UceFj1ZRL6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31 Train the doc2vec embedding model\n",
        "# Model parameters\n",
        "VectorSize = 300 # the number of dimensions into which each word will be embedded.\n",
        "Window = 5 # The number of words before and after the focal word used to train the embedding.\n",
        "Epochs = 10 # The number of iterations through the corpus that the algorithm will perform to train the model.\n",
        "MinCount = 1 # Tells the algorithm to ignore words with a frequency lower than this.\n",
        "Workers = 6 # My machine has 8 processors, so I am assigning six of these to the task of training the model.\n",
        "start = time.time()\n",
        "trained_bigram = list(df.itertuples(index=False, name=None))\n",
        "docs = []\n",
        "for doc in trained_bigram:\n",
        "T = TaggedDocument(doc[0], [doc[1]])\n",
        "docs.append(T)\n",
        "modeldv = Doc2Vec(docs, vector_size = VectorSize, window = Window, epochs = Epochs)\n",
        "print(\"Minutes it took to train the model: \", ((time.time() - start) / 60))\n",
        "modeldv.save('patentAbstractsD2V_300_10_5.model')\n"
      ],
      "metadata": {
        "id": "xJ-ietqYRPKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32 Save the model for future use\n",
        "modeldv = Doc2Vec.load('patentAbstractsD2V_300_10_5.model')\n"
      ],
      "metadata": {
        "id": "6D-_hVDFSB-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33 Compare the similarities for the three patent abstracts in table 2\n",
        "print(modeldv.docvecs.similarity(10000000,8406638))\n",
        "print(modeldv.docvecs.similarity(10000000,10000142))\n",
        "print(modeldv.docvecs.similarity(10000142,8406638))\n"
      ],
      "metadata": {
        "id": "RcTHx9LMSFHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34 Load the dataframe with the patent abstracts and load the embedding model\n",
        "df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "# For our example, we will only calculate the centroid of the abstracts we want\n",
        "df = df.loc[df['id'].isin([10000000, 8406638, 10000142])]\n",
        "df[\"abstract_token_bigram\"] = df[\"abstract_token_bigram\"].apply(lambda x: string_to_ls(x))\n",
        "model = Word2Vec.load('patentAbstractsW2V_300_10_5.model')\n",
        "model_vocab=model.wv.key_to_index\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4SJj98OfSIle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35 Use the centroid vector function in #27 to calculate the centroid vector for the time and geometry archetypes\n",
        "time = ['interval','time','period','preceding','time_interval','start','timing','immediately_preceding','nanosecond','seconds',\n",
        "'minutes','hours','synchronize','synchronized','instant','continuation','duration']\n",
        "time_vector = get_centroid_vector(time)\n",
        "geometry = ['plane','ellipse','parabola','straight_line','bisector','arc_circle','tangent','hyperbolic','tangents','curvature',\n",
        "'angle','circular_arc','sagittal','axis_symmetry','meridian','ellipsoid','paraboloid','regular_polygon','dihedral',\n",
        "'intersection','geometry','geometric']\n",
        "geometry_vector = get_centroid_vector(geometry)\n",
        "\n"
      ],
      "metadata": {
        "id": "upe1EaCfSLp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36 Measure the similarity between thet time and geomtry archetype vectors to our sample patents in table 2\n",
        "# Centroid vectors of our patents\n",
        "v1 = df.loc[df['id'] == 10000000, 'centroid_vector'].iloc[0]\n",
        "v2 = df.loc[df['id'] == 8406638 , 'centroid_vector'].iloc[0]\n",
        "v3 = df.loc[df['id'] == 10000142, 'centroid_vector'].iloc[0]\n",
        "# Compare the second (8406638) and third (10000142) patents to the time archetype\n",
        "print(\"Similarity between the time archetype and a coherent light receiver: \", alt_cosine(v1,time_vector))\n",
        "print(\"Similarity between the time archetype and a coherent light receiver: \", alt_cosine(v2,time_vector))\n",
        "print(\"Similarity between the time archetype and a head and neck restraint: \", alt_cosine(v3,time_vector))\n",
        "print(\"\")\n",
        "# Compare the second (8406638) and third (10000142) patents to the geometry archetype\n",
        "print(\"Similarity between the geometry archetype and a coherent light receiver: \", alt_cosine(v1,geometry_vector))\n",
        "print(\"Similarity between the geometry archetype and a coherent light receiver: \", alt_cosine(v2,geometry_vector))\n",
        "print(\"Similarity between the geometry archetype and a head and neck restraint: \", alt_cosine(v3,geometry_vector))\n",
        "\n"
      ],
      "metadata": {
        "id": "Cl1n3WVbSTHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37 Terms most associated with different brain regions. Note that we only provide these associations for the model trained\n",
        "# on all patents. In practice, one would want to first train separate embedding models based on the cleavages of interest.\n",
        "print(\"hippocampus: \" , model.wv.most_similar('hippocampus', topn=10))\n",
        "print(\"\")\n",
        "print(\"subthalamic_nucleus: \", model.wv.most_similar('subthalamic_nucleus', topn=10))\n",
        "print(\"\")\n",
        "print(\"nucleus_basalis: \" , model.wv.most_similar('nucleus_basalis', topn=10))\n"
      ],
      "metadata": {
        "id": "qUTrJhyKSaxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}