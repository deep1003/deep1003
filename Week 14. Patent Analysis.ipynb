{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTGvSXjmT8J/tWhYlrx2X9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep1003/deep1003/blob/master/Week%2014.%20Patent%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 구글 드라이브의 경로로 이동\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks')\n",
        "\n",
        "# 현재 작업 디렉토리 확인\n",
        "!pwd\n"
      ],
      "metadata": {
        "id": "aQy-7T6hfn2h",
        "outputId": "ff46285e-4701-418c-e710-47441fc2280d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 필수 패키지 설치 명령어 (Colab 환경용)\n",
        "!pip install tqdm pandas numpy matplotlib nltk gensim scikit-learn ksvd\n",
        "\n",
        "# nltk 데이터를 다운로드하는 명령어 (추가적으로 필요할 수 있음)\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "Et7dd2x2fLEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgGglIiKJkDj"
      },
      "outputs": [],
      "source": [
        "# 1. 필수 패키지 및 모듈 임포트\n",
        "import os  # 파일 경로 및 작업 디렉토리 제어\n",
        "import math  # 수학적 연산을 위한 모듈\n",
        "import time  # 시간 측정 및 지연 처리\n",
        "import pickle  # 데이터 직렬화 및 역직렬화를 위한 모듈\n",
        "from tqdm import tqdm  # 진행 상태 표시를 위한 모듈\n",
        "import pandas as pd  # 데이터 분석을 위한 라이브러리 (DataFrame 처리)\n",
        "tqdm.pandas()  # pandas 진행 상태 바 추가\n",
        "import numpy as np  # 수치 연산을 위한 라이브러리\n",
        "import matplotlib.pyplot as plt  # 데이터 시각화를 위한 라이브러리\n",
        "import matplotlib.cm as cm  # 색상 맵 제어\n",
        "from pprint import pprint  # 데이터 구조를 잘 보이게 출력하는 모듈\n",
        "import re  # 정규 표현식을 위한 모듈\n",
        "import nltk  # 자연어 처리 라이브러리\n",
        "import gensim  # 토픽 모델링 및 임베딩 처리\n",
        "from gensim.test.utils import datapath  # Gensim 데이터 경로 유틸리티\n",
        "from gensim.models import Word2Vec  # Word2Vec 모델\n",
        "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS  # 문구 학습 모델\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument  # Doc2Vec 모델\n",
        "from gensim import corpora, models, similarities  # Gensim의 주요 모듈들\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # 코사인 유사도 계산\n",
        "from gensim.models import KeyedVectors  # 사전 학습된 키 임베딩 로드\n",
        "from random import seed, sample  # 랜덤 작업을 위한 모듈\n",
        "import random  # 랜덤 작업을 위한 모듈\n",
        "from ksvd import ApproximateKSVD  # KSVD(희소 코딩 알고리즘)\n",
        "import sklearn  # 머신러닝 관련 라이브러리\n",
        "from sklearn.decomposition import PCA  # 차원 축소를 위한 PCA\n",
        "from sklearn.manifold import TSNE  # 차원 축소를 위한 t-SNE\n",
        "from mpl_toolkits.mplot3d import Axes3D  # 3D 플롯 생성 도구\n",
        "# from sklearn.preprocessing import OneHotEncoder, LabelEncoder  # 인코딩 관련 라이브러리 (주석 처리됨)\n",
        "# from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances  # 유사도 계산 모듈 (주석 처리됨)\n",
        "import warnings  # 경고 무시 모듈\n",
        "warnings.filterwarnings('ignore')  # 경고 메시지 무시 설정\n",
        "from matplotlib.axes._axes import _log as matplotlib_axes_logger  # Matplotlib 경고 제어\n",
        "matplotlib_axes_logger.setLevel('ERROR')  # Matplotlib 로그 레벨 설정 (오류만 출력)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 데이터 로딩 함수, 각 데이터프레임의 셀 문자열을 리스트로 변환하는 함수\n",
        "def string_to_ls(text):\n",
        "    word_ls_strip = []  # 빈 리스트 생성\n",
        "    word_ls = text.strip('][').split(', ')  # 입력 텍스트에서 '[]'를 제거하고, ', '로 분리하여 리스트로 변환\n",
        "\n",
        "    # 각 단어에서 따옴표를 제거하고 리스트에 추가\n",
        "    for w in word_ls:\n",
        "        word_ls_strip.append(w.strip(\"'\"))\n",
        "\n",
        "    return word_ls_strip  # 리스트 반환\n"
      ],
      "metadata": {
        "id": "wvrjvVllKcRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 특허 데이터 다운로드 및 로드\n",
        "\n",
        "# 특허 데이터 다운로드\n",
        "# 먼저 https://patentsview.org/download/data-download-tables 에서 특허 초록 데이터를 다운로드하세요.\n",
        "\n",
        "# 그 후, 특허 데이터셋을 메모리로 읽어들입니다.\n",
        "patent_df = pd.read_csv(\"patent.tsv\", low_memory=False, sep=\"\\t\")  # 특허 데이터를 TSV 파일로 읽어들임 (탭 구분)\n",
        "\n",
        "# 데이터프레임의 크기(행, 열 수) 출력\n",
        "print(patent_df.shape)\n",
        "\n",
        "# 데이터프레임의 첫 5행 출력\n",
        "patent_df.head()\n"
      ],
      "metadata": {
        "id": "qW56qVq-KlNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 특허 초록 텍스트를 정리하는 함수 생성\n",
        "\n",
        "def string_tokenize(text):\n",
        "    workingIter = []  # 토큰을 저장할 리스트 초기화\n",
        "\n",
        "    # 입력이 문자열인지 확인\n",
        "    if isinstance(text, str):\n",
        "        # NLTK를 사용해 텍스트를 토큰화 (단어 단위로 분리)\n",
        "        tokenLst = nltk.word_tokenize(text)\n",
        "\n",
        "        # 소문자로 변환, 알파벳만 남기기 (특수문자와 숫자는 제거)\n",
        "        workingIter = [w.lower() for w in tokenLst if w.isalpha()]\n",
        "\n",
        "    return workingIter  # 정리된 토큰 리스트 반환\n"
      ],
      "metadata": {
        "id": "GXZ7xqltKumx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 전처리된 토큰을 저장할 새로운 열 생성 및 데이터 저장\n",
        "\n",
        "# \"abstract\" 열에 있는 텍스트를 토큰화한 결과를 \"abstract_token\" 열에 저장\n",
        "patent_df[\"abstract_token\"] = patent_df[\"abstract\"].progress_apply(lambda x: string_tokenize(x))\n",
        "\n",
        "# 필요한 열만 선택 (id, type, number, date, title, abstract_token)\n",
        "patent_df = patent_df[['id', 'type', 'number', 'date', 'title', 'abstract_token']]\n",
        "\n",
        "# 전처리된 데이터를 CSV 파일로 저장 (index 없이 저장)\n",
        "patent_df.to_csv(\"patents_processed_tokens.csv\", index=False)\n",
        "\n",
        "# 데이터프레임의 크기 출력\n",
        "print(patent_df.shape)\n",
        "\n",
        "# 데이터프레임의 첫 5행 출력\n",
        "patent_df.head()\n"
      ],
      "metadata": {
        "id": "5y3tP7d4LDqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 이전에 저장한 전처리된 데이터를 로드\n",
        "\n",
        "# \"patents_processed_tokens.csv\" 파일을 읽어들임\n",
        "patent_df = pd.read_csv(\"patents_processed_tokens.csv\", low_memory=False)\n",
        "\n",
        "# 전처리된 \"abstract_token\" 열의 문자열을 리스트로 변환\n",
        "patent_df[\"abstract_token\"] = patent_df[\"abstract_token\"].progress_apply(lambda x: string_to_ls(x))\n"
      ],
      "metadata": {
        "id": "Oyz7MA0gLPbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 학습 코퍼스 가져오기 (모든 특허 텍스트)\n",
        "\n",
        "# \"abstract_token\" 열에서 전처리된 모든 특허 텍스트를 리스트로 변환\n",
        "training_patent = list(patent_df[\"abstract_token\"])\n"
      ],
      "metadata": {
        "id": "STtdhIALLXt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. 모델 학습 (기본 설정 사용: min_count=5, threshold=10)\n",
        "\n",
        "# Phrases 모델 학습 (기본 설정을 사용하여 토큰 간의 빈번한 쌍을 식별)\n",
        "phrase_model = Phrases(training_patent, connector_words=ENGLISH_CONNECTOR_WORDS)\n"
      ],
      "metadata": {
        "id": "FCsvFPEPLcpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. 각 특허 초록의 바이그램(bigram) 버전을 생성\n",
        "\n",
        "# 각 \"abstract_token\" 리스트에서 바이그램 버전의 토큰 리스트를 생성\n",
        "patent_df[\"abstract_token_bigram\"] = patent_df[\"abstract_token\"].progress_apply(lambda x: phrase_model[x])\n"
      ],
      "metadata": {
        "id": "YQRTZ1_OLjyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. 필요한 열만 남기고 데이터프레임을 저장\n",
        "\n",
        "# 필요한 열만 유지 (id, type, number, date, title, abstract_token_bigram)\n",
        "patent_df = patent_df[['id', 'type', 'number', 'date', 'title', 'abstract_token_bigram']]\n"
      ],
      "metadata": {
        "id": "hZpS_c8YLooP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. 이전에 저장한 전처리된 바이그램 데이터를 로드\n",
        "\n",
        "# \"patents_processed_bigrams.csv\" 파일에서 데이터 로드\n",
        "patent_df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "\n",
        "# 문자열 형태의 리스트를 실제 리스트로 변환\n",
        "patent_df[\"abstract_token_bigram\"] = patent_df[\"abstract_token_bigram\"].progress_apply(lambda x: string_to_ls(x))\n"
      ],
      "metadata": {
        "id": "tRDSuj0CLss4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. 바이그램이 포함된 특허 초록 열을 알고리즘에 입력하기 위해 리스트로 변환\n",
        "\n",
        "# \"abstract_token_bigram\" 열의 데이터를 리스트로 변환하여 모델에 입력할 준비\n",
        "trained_bigram = list(patent_df[\"abstract_token_bigram\"])\n"
      ],
      "metadata": {
        "id": "0M5QsYIaPCef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. 모델 파라미터 설정 및 특허 초록 리스트를 통한 모델 학습\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "VectorSize = 300  # 각 단어를 임베딩할 차원의 수\n",
        "Window = 5  # 초점 단어 주변의 단어 개수 (앞뒤로 5개씩)\n",
        "Epochs = 10  # 코퍼스를 학습할 반복 횟수\n",
        "MinCount = 1  # 해당 빈도보다 적게 등장하는 단어는 무시\n",
        "Workers = 6  # 병렬 처리할 작업자 수 (6개 프로세서 사용)\n",
        "\n",
        "# 모델 학습 시작 시간 기록\n",
        "start = time.time()\n",
        "\n",
        "# Word2Vec 모델 학습 (CBOW 알고리즘 사용)\n",
        "w2v_model = Word2Vec(sentences=trained_bigram, vector_size=VectorSize, window=Window,\n",
        "                     min_count=MinCount, workers=Workers, epochs=Epochs)\n",
        "\n",
        "# 학습에 걸린 시간 출력 (분 단위)\n",
        "print(\"Minutes it took to train the model: \", (time.time() - start) / 60)\n",
        "\n",
        "# 학습된 Word2Vec 모델 저장\n",
        "w2v_model.save('patentAbstractsW2V_300_10_5.model')\n"
      ],
      "metadata": {
        "id": "7mnzjBAzPLRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. 모델이 학습되고 하드 드라이브에 저장된 후, 이를 불러와서 사용\n",
        "\n",
        "# 저장된 Word2Vec 모델 불러오기\n",
        "model = Word2Vec.load('patentAbstractsW2V_300_10_5.model')\n"
      ],
      "metadata": {
        "id": "qXb5jSZbPgNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. 모델 검증: 로컬 개념 구조가 직관과 일치하는지 확인\n",
        "\n",
        "# focal word(중심 단어)에 대해 가장 유사한 단어 10개를 반환\n",
        "# 이 단어들과 focal word 간의 코사인 유사도를 계산하여 반환함 (코사인 값이 1에 가까울수록 유사도가 높음)\n",
        "\n",
        "# 'light' 단어와 가장 유사한 단어 10개 출력\n",
        "print(model.wv.most_similar('light'))\n",
        "print(\"\")\n",
        "\n",
        "# 'chemical' 단어와 가장 유사한 단어 10개 출력\n",
        "print(model.wv.most_similar('chemical'))\n",
        "print(\"\")\n",
        "\n",
        "# 'car' 단어와 가장 유사한 단어 10개 출력\n",
        "print(model.wv.most_similar('car'))\n"
      ],
      "metadata": {
        "id": "daE394Z3PoXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. 모델 검증: 글로벌 개념 구조가 로컬 구조와 일치하는지 확인\n",
        "\n",
        "# 단어 간 코사인 유사도를 통해 단어들이 전체 개념 공간에서도 적절한 거리를 유지하는지 확인\n",
        "# 유사도가 높은 단어와 유사도가 낮은 단어를 비교하여 글로벌 구조를 평가\n",
        "\n",
        "# 'chemical'과 다른 단어들 간의 코사인 유사도 계산\n",
        "print(model.wv.similarity('chemical', 'biological'))  # 'chemical' vs. 'biological'\n",
        "print(model.wv.similarity('chemical', 'drug'))  # 'chemical' vs. 'drug'\n",
        "print(model.wv.similarity('chemical', 'food'))  # 'chemical' vs. 'food'\n",
        "print(model.wv.similarity('chemical', 'engineering'))  # 'chemical' vs. 'engineering'\n",
        "print(model.wv.similarity('chemical', 'software'))  # 'chemical' vs. 'software'\n",
        "print(model.wv.similarity('chemical', 'car'))  # 'chemical' vs. 'car'\n"
      ],
      "metadata": {
        "id": "mLk65o-NPr32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. 2차원 시각화로 로컬 및 글로벌 구조 확인\n",
        "\n",
        "# 주요 단어들 정의 (중심 단어)\n",
        "keys = ['computer', 'telephone', 'car', 'boat', 'drug', 'chemical']\n",
        "\n",
        "# 단어 및 임베딩 클러스터 초기화\n",
        "embedding_clusters = []\n",
        "word_clusters = []\n",
        "\n",
        "# 각 중심 단어에 대해 가장 유사한 단어들을 클러스터링\n",
        "for word in keys:\n",
        "    embeddings = []\n",
        "    words = []\n",
        "    # 모델을 사용해 중심 단어와 유사한 단어 6개를 추출\n",
        "    for similar_word, _ in model.wv.most_similar(word, topn=6):\n",
        "        words.append(similar_word)\n",
        "        embeddings.append(model.wv[similar_word])\n",
        "    embedding_clusters.append(embeddings)\n",
        "    word_clusters.append(words)\n",
        "\n",
        "# t-SNE 파라미터 설정\n",
        "perp = 9  # perplexity 값 설정 (데이터 밀집도에 영향을 미침)\n",
        "embedding_clusters = np.array(embedding_clusters)\n",
        "n, m, k = embedding_clusters.shape\n",
        "\n",
        "# t-SNE로 차원 축소 수행 (768차원 -> 2차원)\n",
        "tsne_model_en_2d = TSNE(perplexity=perp, n_components=2, init='pca', n_iter=50000, random_state=32)\n",
        "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
        "\n",
        "# 시각화 함수 정의\n",
        "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))  # 클러스터별 색상 설정\n",
        "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
        "        x = embeddings[:, 0]\n",
        "        y = embeddings[:, 1]\n",
        "        plt.scatter(x, y, c=color, alpha=a, label=label, s=400)  # 클러스터 점 그리기\n",
        "        for i, word in enumerate(words):\n",
        "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 8),\n",
        "                         textcoords='offset points', ha='right', va='bottom', size=16)  # 단어 주석 추가\n",
        "    plt.legend(loc=4, prop={'size': 16})\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    plt.grid(False)  # 그리드 제거\n",
        "    if filename:\n",
        "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')  # 파일 저장\n",
        "    plt.show()\n",
        "\n",
        "# 결과를 파일로 저장 및 시각화\n",
        "outfile = \"G:\\\\My Drive\\\\Projects\\\\0 Word Embeddings OS\\\\Embeddings Appendix\\\\TSNE.png\"\n",
        "tsne_plot_similar_words('Word Similarities from Patent Abstracts', keys, embeddings_en_2d, word_clusters, 0.7, outfile)\n",
        "\n",
        "# 플롯 설명:\n",
        "# 각 중심 단어에 대해 유사한 단어들이 같은 클러스터 내에 위치하며, 관련 있는 클러스터들끼리 가까운 곳에 위치합니다.\n",
        "# 예: computer와 telephone 클러스터, car와 boat 클러스터, drug와 chemical 클러스터가 각각 서로 가까운 위치에 있음.\n"
      ],
      "metadata": {
        "id": "PUeHdrAaPyB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. 기존의 특허 카테고리 구조와 모델 출력을 비교하여 구조적 일관성 확인\n",
        "\n",
        "# 특정 단어들 간의 코사인 유사도를 계산하고, 이를 0에서 100 사이의 값으로 변환\n",
        "# 이 값은 거리로 해석되며, 값이 클수록 두 단어 간의 거리가 더 멀다고 판단\n",
        "\n",
        "# 유사도를 거리 값으로 변환하여 출력\n",
        "print((- (model.wv.similarity('cement', 'nanotechnology') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'plastics') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'yarn') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'spraying') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'water') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'printing') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'lighting') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'vehicles') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('cement', 'veterinary') + 1) * 50) + 100)\n",
        "\n",
        "# 거리 값 설명:\n",
        "# 여기서 유사도 값 (-1 ~ 1 사이)을 0에서 100 사이로 변환합니다.\n",
        "# 유사도가 높으면(가깝다면) 값이 작고, 유사도가 낮으면(멀다면) 값이 큽니다.\n"
      ],
      "metadata": {
        "id": "B2_TaZ3QP3pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. 모델이 물리적 세계의 객관적 규칙성을 포착하는지 확인\n",
        "# 원소 주기율표에서 물리적 거리를 모델이 어떻게 표현하는지 확인하기 위해 유사도를 거리로 변환\n",
        "\n",
        "# 'hydrogen'과 다른 알칼리 금속 원소 간의 유사도를 거리 값으로 변환하여 출력\n",
        "print((- (model.wv.similarity('hydrogen', 'lithium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'sodium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'potassium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'rubidium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'caesium') + 1) * 50) + 100)\n",
        "print((- (model.wv.similarity('hydrogen', 'francium') + 1) * 50) + 100)\n"
      ],
      "metadata": {
        "id": "wIAgyFwBQPGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. 특허 임베딩 모델이 사회적 관계와 관련된 지식을 잘 포착하는지 확인\n",
        "# 여기서는 성별을 기준으로 개념이 여성/남성 차원에 맞게 투영되는지 확인\n",
        "\n",
        "# 벡터 정규화 함수\n",
        "def normalize(vector):\n",
        "    normalized_vector = vector / np.linalg.norm(vector)  # 벡터의 크기를 1로 만듦 (정규화)\n",
        "    return normalized_vector\n",
        "\n",
        "# 특정 차원을 정의하는 함수 (성별 차원)\n",
        "def dimension(model, positives, negatives):\n",
        "    # 긍정적 단어와 부정적 단어의 벡터 차이를 계산하여 차원 생성\n",
        "    diff = sum([normalize(model.wv[x]) for x in positives]) - sum([normalize(model.wv[y]) for y in negatives])\n",
        "    return diff\n",
        "\n",
        "# 성별 차원 정의 ('man', 'male' vs. 'woman', 'female')\n",
        "Gender = dimension(model, ['man', 'him', 'he', 'male', 'men'], ['woman', 'her', 'she', 'female', 'women'])\n",
        "\n",
        "# 확인할 개념 리스트 정의\n",
        "Concepts = ['dietitian', 'hygienist', 'lipstick', 'breastpump', 'tampon',\n",
        "            'military', 'farming', 'police', 'hammer', 'fishing']\n",
        "\n",
        "# 개념을 성별 차원에 투영하여 데이터프레임 생성\n",
        "def makeDF(model, word_list):\n",
        "    g = []  # 성별 투영 값을 저장할 리스트\n",
        "    for word in word_list:\n",
        "        # 각 단어가 성별 차원에 얼마나 투영되는지 계산 (코사인 유사도)\n",
        "        g.append(sklearn.metrics.pairwise.cosine_similarity(model.wv[word].reshape(1, -1), Gender.reshape(1, -1))[0][0])\n",
        "    # 데이터프레임 생성 (단어별 성별 투영 값)\n",
        "    df = pd.DataFrame({'gender': g}, index=word_list)\n",
        "    return df\n",
        "\n",
        "# 개념 리스트를 성별 차원에 투영한 데이터프레임 생성 및 정렬\n",
        "df = makeDF(model, Concepts)\n",
        "df = df.sort_values(by=['gender'])  # 성별 투영 값을 기준으로 정렬\n",
        "\n",
        "# 결과 출력\n",
        "df\n"
      ],
      "metadata": {
        "id": "GT8D5sslQGZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. 임베딩 모델이 아날로지 추론 작업을 해결할 수 있는지 검증\n",
        "# 모델이 잘 학습되었다면 아날로지 작업에서 의미 있는 결과를 도출할 수 있음\n",
        "\n",
        "# 모델의 코사인 유사도를 사용해 아날로지 추론 수행 (cosmul 방식 사용)\n",
        "# 각 출력은 아날로지에서 유사도가 가장 높은 단어를 반환함\n",
        "\n",
        "# Synonym 추론: 'tiny'는 'large'처럼 크기와 관련된 반대 단어를 나타내므로 'big'을 뺀 결과로 유사한 단어를 반환\n",
        "print(model.wv.most_similar_cosmul(positive=['tiny', 'large'], negative=['big'])[0])\n",
        "\n",
        "# Antonym 추론: 'downward'는 'liquid'와 관련 있고, 'solid'와 반대인 유사 단어를 반환\n",
        "print(model.wv.most_similar_cosmul(positive=['downward', 'liquid'], negative=['solid'])[0])\n",
        "\n",
        "# Type 추론: 'car'는 'animal'과 관련 있으며, 'cat'과 유사하지 않은 단어를 반환\n",
        "print(model.wv.most_similar_cosmul(positive=['car', 'animal'], negative=['cat'])[0])\n",
        "\n",
        "# Type 추론: 'skate'와 'airplane' 관련 단어 추론, 'fly'를 제거한 결과에서 두 번째 유사 단어를 반환\n",
        "print(model.wv.most_similar_cosmul(positive=['skate', 'airplane'], negative=['fly'])[1])\n",
        "\n",
        "# Item / Purpose 추론: 'hammer'와 'cut' 관련 단어 추론, 'knife' 제거 후 유사 단어 반환\n",
        "print(model.wv.most_similar_cosmul(positive=['hammer', 'cut'], negative=['knife'])[0])\n",
        "\n",
        "# Product / Worker 추론: 'farmer'와 'patient'의 관련성 추론, 'doctor'를 제거한 결과 유사 단어 반환\n",
        "print(model.wv.most_similar_cosmul(positive=['farmer', 'patient'], negative=['doctor'])[0])\n",
        "\n",
        "# Distance 추론: 'short'와 'longest'의 관련성 추론, 'long'을 제거한 유사 단어 반환\n",
        "print(model.wv.most_similar_cosmul(positive=['short', 'longest'], negative=['long'])[0])\n",
        "\n",
        "# Count 추론: 'spoon'과 'pens'의 관련성 추론, 'pen'을 제거한 유사 단어 반환\n",
        "print(model.wv.most_similar_cosmul(positive=['spoon', 'pens'], negative=['pen'])[0])\n"
      ],
      "metadata": {
        "id": "oAgL4lTeQaNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. 특허 초록 데이터프레임 및 임베딩 모델 로드\n",
        "\n",
        "# 'patents_processed_bigrams.csv' 파일에서 특허 데이터를 로드\n",
        "df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "\n",
        "# 실용적인 예시를 위해 상위 5개의 특허 초록만 사용\n",
        "df = df.head()\n",
        "\n",
        "# 'abstract_token_bigram' 열에 저장된 문자열을 실제 리스트로 변환\n",
        "df[\"abstract_token_bigram\"] = df[\"abstract_token_bigram\"].progress_apply(lambda x: string_to_ls(x))\n",
        "\n",
        "# 저장된 Word2Vec 임베딩 모델 로드\n",
        "model = Word2Vec.load('patentAbstractsW2V_300_10_5.model')\n",
        "\n",
        "# 데이터프레임의 첫 5행 출력\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "NZ1pDhRsQjyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. 각 특허 초록을 출력하여 내용을 확인\n",
        "\n",
        "# 'abstract_token_bigram' 열의 데이터를 리스트로 변환\n",
        "abstract_list = df['abstract_token_bigram'].tolist()\n",
        "\n",
        "# 각 특허 초록을 출력\n",
        "for abstract in abstract_list:\n",
        "    print(\"\")  # 각 초록 사이에 빈 줄 삽입\n",
        "    print(len(abstract))  # 초록의 토큰 수 출력\n",
        "    print(abstract)  # 초록 내용 출력\n"
      ],
      "metadata": {
        "id": "7bUKPJq8Qpd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. 개념적 범위 측정\n",
        "# 값이 클수록 범위가 넓어지므로, 유사도 값을 변환하여 범위를 반영\n",
        "\n",
        "# 두 개의 단어 집합 정의\n",
        "l1 = [\"chemistry\", \"biochemistry\", \"analytical_chemistry\"]\n",
        "l2 = [\"chemistry\", \"oceanography\", \"computer\"]\n",
        "\n",
        "# 첫 번째 단어 집합의 유사도 계산\n",
        "sims1 = [\n",
        "    model.wv.similarity(l1[0], l1[1]),  # 'chemistry'와 'biochemistry' 간 유사도\n",
        "    model.wv.similarity(l1[0], l1[2]),  # 'chemistry'와 'analytical_chemistry' 간 유사도\n",
        "    model.wv.similarity(l1[1], l1[2])   # 'biochemistry'와 'analytical_chemistry' 간 유사도\n",
        "]\n",
        "\n",
        "# 첫 번째 집합의 유사도의 평균을 계산하고, 변환하여 범위를 반영 (값이 클수록 범위가 넓음)\n",
        "sims1mean = (((sum(sims1) / float(len(sims1)) + 1) * -50) + 100)  # 유사도 평균을 거리로 변환\n",
        "\n",
        "# 첫 번째 단어 집합과 범위 출력\n",
        "print(l1)\n",
        "print(sims1mean)\n",
        "print(\"\")\n",
        "\n",
        "# 두 번째 단어 집합의 유사도 계산\n",
        "sims2 = [\n",
        "    model.wv.similarity(l2[0], l2[1]),  # 'chemistry'와 'oceanography' 간 유사도\n",
        "    model.wv.similarity(l2[0], l2[2]),  # 'chemistry'와 'computer' 간 유사도\n",
        "    model.wv.similarity(l2[1], l2[2])   # 'oceanography'와 'computer' 간 유사도\n",
        "]\n",
        "\n",
        "# 두 번째 집합의 유사도의 평균을 계산하고, 변환하여 범위를 반영 (값이 클수록 범위가 넓음)\n",
        "sims2mean = (((sum(sims2) / float(len(sims2)) + 1) * -50) + 100)  # 유사도 평균을 거리로 변환\n",
        "\n",
        "# 두 번째 단어 집합과 범위 출력\n",
        "print(l2)\n",
        "print(sims2mean)\n"
      ],
      "metadata": {
        "id": "BWYMMTiWQs4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. 주어진 문서에서 100개의 임의의 단어 쌍을 샘플링하고 평균 유사도를 계산하는 함수\n",
        "# 많은 단어가 있는 문서에서 계산 부담을 줄이기 위해 유용\n",
        "\n",
        "def conceptual_breadth(text):\n",
        "    \"\"\"\n",
        "    주어진 텍스트에서 100개의 임의의 단어 쌍을 샘플링하여 평균 유사도를 계산하는 함수.\n",
        "    계산이 어려운 경우, 대신 단어-단어 간 코사인 거리의 평균을 사용할 수 있음.\n",
        "    \"\"\"\n",
        "    numUnique = len(text)  # 텍스트 내 고유 단어 수\n",
        "    distances = []\n",
        "\n",
        "    for i in range(100):\n",
        "        try:\n",
        "            # 임의의 두 단어를 샘플링\n",
        "            rand1 = text[random.randrange(0, numUnique)]\n",
        "            rand2 = text[random.randrange(0, numUnique)]\n",
        "\n",
        "            # 두 단어 간 코사인 유사도를 거리로 변환\n",
        "            dist = (((model.wv.similarity(rand1, rand2) + 1) * -50) + 100)  # 값이 클수록 더 넓은 개념적 범위를 의미\n",
        "            distances.append(dist)\n",
        "        except:\n",
        "            continue  # 예외 발생 시 무시하고 계속 진행\n",
        "\n",
        "    try:\n",
        "        # 계산된 거리 값 리스트에서 평균 계산\n",
        "        dists = distances\n",
        "        dists.sort(reverse=True)  # 내림차순으로 정렬 (옵션)\n",
        "        breadth = float(sum(dists)) / float(len(dists))  # 평균 거리 계산\n",
        "    except:\n",
        "        breadth = np.nan  # 예외 발생 시 NaN 값 할당\n",
        "\n",
        "    return breadth  # 개념적 범위 반환\n",
        "\n",
        "# 각 특허 초록에 대해 개념적 범위를 계산하고 새로운 열에 저장\n",
        "df['abstract_breadth'] = df['abstract_token_bigram'].apply(conceptual_breadth)\n",
        "\n",
        "# 데이터프레임의 첫 5행 출력\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "ePgZbncTQ2DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. 특허 초록 데이터프레임 및 임베딩 모델 로드\n",
        "\n",
        "# 'patents_processed_bigrams.csv' 파일에서 데이터 로드\n",
        "df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "\n",
        "# 'abstract_token_bigram' 열의 문자열을 리스트로 변환\n",
        "df[\"abstract_token_bigram\"] = df[\"abstract_token_bigram\"].progress_apply(lambda x: string_to_ls(x))\n",
        "\n",
        "# 저장된 Word2Vec 임베딩 모델 로드\n",
        "model = Word2Vec.load('patentAbstractsW2V_300_10_5.model')\n",
        "\n",
        "# 모델의 어휘(vocab) 로드\n",
        "model_vocab = model.wv.key_to_index\n",
        "\n",
        "# 데이터프레임의 첫 5행 출력\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "1BOcand7Q8f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27. 특허 초록의 중심 벡터(centroid vector)를 계산하는 함수\n",
        "def get_centroid_vector(token_ls):\n",
        "    total_vector = []  # 전체 벡터를 저장할 리스트\n",
        "\n",
        "    # 각 토큰에 대해 벡터를 추출하고, 모델 어휘에 있는 경우 벡터를 리스트에 추가\n",
        "    for token in token_ls:\n",
        "        if token in model_vocab:\n",
        "            v = model.wv[token]\n",
        "            total_vector.append(v)\n",
        "\n",
        "    # 벡터가 하나 이상 있는 경우 평균을 계산하여 중심 벡터 생성\n",
        "    if len(total_vector) > 0:\n",
        "        centroid_vector = sum(total_vector) / len(total_vector)\n",
        "    else:\n",
        "        centroid_vector = np.nan  # 벡터가 없으면 NaN 반환\n",
        "\n",
        "    return centroid_vector  # 중심 벡터 반환\n",
        "\n",
        "# 더 빠르게 코사인 유사도를 계산하는 함수\n",
        "def alt_cosine(x, y):\n",
        "    return np.inner(x, y) / np.sqrt(np.dot(x, x) * np.dot(y, y))\n"
      ],
      "metadata": {
        "id": "zTwYcisJQ_WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28. 테이블 2의 각 특허 초록에 대해 중심 벡터(centroid vector) 계산\n",
        "# 예시에서는 특정 특허 초록에 대해서만 중심 벡터를 계산함\n",
        "\n",
        "# id가 10000000, 8406638, 10000142인 특허 초록만 필터링\n",
        "df = df.loc[df['id'].isin([10000000, 8406638, 10000142])]\n",
        "\n",
        "# 각 특허 초록에 대해 중심 벡터를 계산하여 새로운 'centroid_vector' 열에 저장\n",
        "df['centroid_vector'] = df['abstract_token_bigram'].progress_apply(lambda x: get_centroid_vector(x))\n",
        "\n",
        "# 데이터프레임의 첫 5행 출력\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "iNa03M6SREy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. 세 개의 특허 초록에 대한 유사도 비교 (테이블 2)\n",
        "\n",
        "# 각 특허 초록의 중심 벡터 추출\n",
        "v1 = df.loc[df['id'] == 10000000, 'centroid_vector'].iloc[0]\n",
        "v2 = df.loc[df['id'] == 8406638 , 'centroid_vector'].iloc[0]\n",
        "v3 = df.loc[df['id'] == 10000142, 'centroid_vector'].iloc[0]\n",
        "\n",
        "# 중심 벡터 간 코사인 유사도 비교\n",
        "print(alt_cosine(v1, v2))  # 10000000과 8406638 특허 초록 간의 유사도\n",
        "print(alt_cosine(v1, v3))  # 10000000과 10000142 특허 초록 간의 유사도\n",
        "print(alt_cosine(v3, v2))  # 10000142와 8406638 특허 초록 간의 유사도\n"
      ],
      "metadata": {
        "id": "I1HTAjsDRH9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30. 이전에 저장한 전처리된 데이터를 로드\n",
        "\n",
        "# 'patents_processed_bigrams.csv' 파일에서 데이터 로드\n",
        "df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "\n",
        "# 필요한 열만 선택 ('abstract_token_bigram', 'id')\n",
        "df = df[[\"abstract_token_bigram\", \"id\"]]\n",
        "\n",
        "# 'abstract_token_bigram' 열의 문자열을 리스트로 변환\n",
        "df[\"abstract_token_bigram\"] = df[\"abstract_token_bigram\"].progress_apply(lambda x: string_to_ls(x))\n"
      ],
      "metadata": {
        "id": "5UceFj1ZRL6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Doc2Vec 임베딩 모델 학습\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "VectorSize = 300  # 각 단어를 임베딩할 차원의 수\n",
        "Window = 5  # 초점 단어 앞뒤로 고려할 단어 수\n",
        "Epochs = 10  # 코퍼스를 학습할 반복 횟수\n",
        "MinCount = 1  # 최소 등장 횟수가 1 이상인 단어만 포함\n",
        "Workers = 6  # 6개의 프로세서 사용 (8개의 CPU 중 6개 할당)\n",
        "\n",
        "# 모델 학습 시작 시간 기록\n",
        "start = time.time()\n",
        "\n",
        "# 학습에 사용할 문서 리스트 생성\n",
        "trained_bigram = list(df.itertuples(index=False, name=None))  # DataFrame에서 튜플 형태로 데이터 추출\n",
        "docs = []\n",
        "\n",
        "# 각 문서를 TaggedDocument 형식으로 변환\n",
        "for doc in trained_bigram:\n",
        "    T = TaggedDocument(doc[0], [doc[1]])  # 문서의 텍스트와 그 ID를 태그로 연결\n",
        "    docs.append(T)\n",
        "\n",
        "# Doc2Vec 모델 학습\n",
        "modeldv = Doc2Vec(docs, vector_size=VectorSize, window=Window, epochs=Epochs, workers=Workers)\n",
        "\n",
        "# 학습에 걸린 시간 출력 (분 단위)\n",
        "print(\"Minutes it took to train the model: \", ((time.time() - start) / 60))\n",
        "\n",
        "# 학습된 모델 저장\n",
        "modeldv.save('patentAbstractsD2V_300_10_5.model')\n"
      ],
      "metadata": {
        "id": "xJ-ietqYRPKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32. Doc2Vec 모델을 로드하여 나중에 사용할 수 있도록 준비\n",
        "\n",
        "# 저장된 Doc2Vec 모델 로드\n",
        "modeldv = Doc2Vec.load('patentAbstractsD2V_300_10_5.model')\n"
      ],
      "metadata": {
        "id": "6D-_hVDFSB-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33. 테이블 2의 세 개의 특허 초록 간 유사도 비교\n",
        "\n",
        "# Doc2Vec 모델을 사용하여 세 개의 특허 초록 간 유사도 계산 및 출력\n",
        "print(modeldv.dv.similarity(10000000, 8406638))   # 특허 10000000과 8406638 간의 유사도\n",
        "print(modeldv.dv.similarity(10000000, 10000142))  # 특허 10000000과 10000142 간의 유사도\n",
        "print(modeldv.dv.similarity(10000142, 8406638))   # 특허 10000142와 8406638 간의 유사도\n"
      ],
      "metadata": {
        "id": "RcTHx9LMSFHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34. 특허 초록 데이터프레임 및 임베딩 모델 로드\n",
        "\n",
        "# 'patents_processed_bigrams.csv' 파일에서 데이터 로드\n",
        "df = pd.read_csv(\"patents_processed_bigrams.csv\")\n",
        "\n",
        "# 예시로 사용할 특정 특허 초록만 필터링\n",
        "df = df.loc[df['id'].isin([10000000, 8406638, 10000142])]\n",
        "\n",
        "# 'abstract_token_bigram' 열의 문자열을 리스트로 변환\n",
        "df[\"abstract_token_bigram\"] = df[\"abstract_token_bigram\"].apply(lambda x: string_to_ls(x))\n",
        "\n",
        "# 저장된 Word2Vec 모델 로드\n",
        "model = Word2Vec.load('patentAbstractsW2V_300_10_5.model')\n",
        "\n",
        "# 모델의 어휘 로드\n",
        "model_vocab = model.wv.key_to_index\n",
        "\n",
        "# 데이터프레임의 첫 5행 출력\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4SJj98OfSIle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35. 'time' 및 'geometry' 아키타입에 대해 중심 벡터 계산\n",
        "\n",
        "# 'time' 아키타입을 구성하는 단어 리스트 정의\n",
        "time = ['interval', 'time', 'period', 'preceding', 'time_interval', 'start', 'timing', 'immediately_preceding',\n",
        "        'nanosecond', 'seconds', 'minutes', 'hours', 'synchronize', 'synchronized', 'instant', 'continuation', 'duration']\n",
        "\n",
        "# 'geometry' 아키타입을 구성하는 단어 리스트 정의\n",
        "geometry = ['plane', 'ellipse', 'parabola', 'straight_line', 'bisector', 'arc_circle', 'tangent', 'hyperbolic',\n",
        "            'tangents', 'curvature', 'angle', 'circular_arc', 'sagittal', 'axis_symmetry', 'meridian',\n",
        "            'ellipsoid', 'paraboloid', 'regular_polygon', 'dihedral', 'intersection', 'geometry', 'geometric']\n",
        "\n",
        "# 'time' 아키타입의 중심 벡터 계산\n",
        "time_vector = get_centroid_vector(time)\n",
        "\n",
        "# 'geometry' 아키타입의 중심 벡터 계산\n",
        "geometry_vector = get_centroid_vector(geometry)\n",
        "\n",
        "# 결과 확인을 위한 출력\n",
        "print(\"Time Centroid Vector:\", time_vector)\n",
        "print(\"Geometry Centroid Vector:\", geometry_vector)\n"
      ],
      "metadata": {
        "id": "upe1EaCfSLp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36. 특허 초록의 중심 벡터와 'time' 및 'geometry' 아키타입 벡터 간의 유사도 측정\n",
        "\n",
        "# 테이블 2의 세 개의 특허 초록 중심 벡터 추출\n",
        "v1 = df.loc[df['id'] == 10000000, 'centroid_vector'].iloc[0]\n",
        "v2 = df.loc[df['id'] == 8406638 , 'centroid_vector'].iloc[0]\n",
        "v3 = df.loc[df['id'] == 10000142, 'centroid_vector'].iloc[0]\n",
        "\n",
        "# 'time' 아키타입 벡터와 특허 초록 간의 유사도 비교\n",
        "print(\"Similarity between the time archetype and patent 10000000 (coherent light receiver): \", alt_cosine(v1, time_vector))\n",
        "print(\"Similarity between the time archetype and patent 8406638 (coherent light receiver): \", alt_cosine(v2, time_vector))\n",
        "print(\"Similarity between the time archetype and patent 10000142 (head and neck restraint): \", alt_cosine(v3, time_vector))\n",
        "print(\"\")\n",
        "\n",
        "# 'geometry' 아키타입 벡터와 특허 초록 간의 유사도 비교\n",
        "print(\"Similarity between the geometry archetype and patent 10000000 (coherent light receiver): \", alt_cosine(v1, geometry_vector))\n",
        "print(\"Similarity between the geometry archetype and patent 8406638 (coherent light receiver): \", alt_cosine(v2, geometry_vector))\n",
        "print(\"Similarity between the geometry archetype and patent 10000142 (head and neck restraint): \", alt_cosine(v3, geometry_vector))\n"
      ],
      "metadata": {
        "id": "Cl1n3WVbSTHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37. 특정 뇌 영역과 가장 관련된 용어 추출\n",
        "# 'hippocampus', 'subthalamic_nucleus', 'nucleus_basalis'에 가장 유사한 상위 10개의 단어를 출력\n",
        "\n",
        "# 'hippocampus'와 가장 유사한 상위 10개 용어 출력\n",
        "print(\"hippocampus: \", model.wv.most_similar('hippocampus', topn=10))\n",
        "print(\"\")\n",
        "\n",
        "# 'subthalamic_nucleus'와 가장 유사한 상위 10개 용어 출력\n",
        "print(\"subthalamic_nucleus: \", model.wv.most_similar('subthalamic_nucleus', topn=10))\n",
        "print(\"\")\n",
        "\n",
        "# 'nucleus_basalis'와 가장 유사한 상위 10개 용어 출력\n",
        "print(\"nucleus_basalis: \", model.wv.most_similar('nucleus_basalis', topn=10))\n"
      ],
      "metadata": {
        "id": "qUTrJhyKSaxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}